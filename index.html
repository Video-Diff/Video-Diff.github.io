<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training">
  <meta name="keywords" content="Discrete Diffusion, Large-Scale Actionless, Video-Diff">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training</title>

  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  <div class="root-div">  
      <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training</h1>
                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <b>Anonymous Authors</b>
                  </span>
                <!--
                <div class="column has-text-centered">
                  <div class="publication-links">

                    <span class="link-block">
                      <a href="#"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="#"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="#"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                    </span>

                  </div>
                  -->
                  <div class="column has-text-centered">
                    <span class="link-block">
                      <a href="#"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code will come soon</span>
                        </a>
                    </span>
                        </div>

                  
                </div>

              </div>
            </div>
          </div>
        </div>
      </section>
<div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-5">Our trained agent VPDD can complete a wide range of tasks in MetaWorld and RLBench:</h2>
        </div>
    </div>
      <section class="section">
  <div class="content has-text-justified">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
           <div class="item basketball">
            <video poster="" id="basketball" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/basketball.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item box-close">
            <video poster="" id="box-close" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/box-close.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item lever-pull">
            <video poster="" id="lever-pull" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/lever-pull.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item pick-out-of-hole">
            <video poster="" id="pick-out-of-hole" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/pick-out-of-hole.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item stick-push">
            <video poster="" id="stick-push" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/stick-push.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item window_close">
            <video poster="" id="window_close" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/window_close.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item sweep-into">
            <video poster="" id="sweep-into" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/sweep-into.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div>
      </section>
          <section class="section">
          <div class="content has-text-justified">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <div id="results-rlbench" class="carousel results-carousel">
           <div class="item close-jar">
            <video poster="" id="close-jar" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_close_jar.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item meat-off-grill">
            <video poster="" id="meat-off-grill" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_meat-off-grill.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item place-cups">
            <video poster="" id="place-cups" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_place-cups.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item put-cupboard">
            <video poster="" id="put-cupboard" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_put-cupboard.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item put-ring">
            <video poster="" id="put-ring" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_put_ring.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item screw_bulb">
            <video poster="" id="screw_bulb" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_screw_bulb.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item slide-block">
            <video poster="" id="slide-block" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_slide-block.mp4"
                      type="video/mp4">
            </video>
          </div>
            <div class="item stack-wine">
            <video poster="" id="stack-wine" autoplay controls muted loop playsinline height="90%">
              <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_stack-wine.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div>
          </section>
      <section class="section" style="padding-bottom: 0.5rem;">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p>
                  Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. 
                  In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. However, it remains a challenge due to the domain gap between humans and robots. 
                  Moreover, it is difficult to extract useful information representing the dynamic world from human videos, because of its noisy and multimodal data structure.
                </p>
                <p>
                  In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine 
                  generative pre-training on human videos and policy fine-tuning on a small number of action-labeled
                  robot videos. We start by learning compressed visual representations from both human and robot
                  videos to obtain unified video tokens. In the pretraining stage, we employ a discrete diffusion
                  model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space.
                </p>
                <p>
                  In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning 
                  trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity 
                  future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches.
                </p>
                <div class="content" style="margin-top: 30px;">
                    <img src="./static/images/intro.jpg" alt="" srcset="" />
                  </div>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->
          <hr>
          <!-- Method. -->
          <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Method</h2>
                <div class="content align-text">
                  <p>
                    The overall pipeline of VPDD. A <strong>video-based VQ-VAE</strong> is leveraged to encode both human and robot videos into discrete latent
                    codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised 
                    objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction 
                    model \(p_{\theta_1}\) can capture temporal dynamics and representations of the current task. Lastly, we fine-tune our diffusion model on a limited 
                    number of robot data. In each diffusion step of the fine-tuning stage, we leverage \(p_{\theta_1}\) to provide hidden representations \(z_{\widetilde{x}_{0}^{v}}\)
                    to accelerate downstream action learning with video foresight. Both video prediction and action learning are executed simultaneously 
                    through our unified objective.
                    </p>
                </div>

              </div>
          </div>
            <div class="content" style="margin-top: 30px;">
                  <img src="./static/images/method.jpg" alt="" srcset="" />
            </div>
          <hr>
          <!--/ Method. -->
        </div>
      </section>

      <section class="section" style="padding-top: 1rem;">
        <div class="container is-max-desktop">
          
          <div class="content align-text">
            <div class="content">
            VPDD can generate consistent videos in the egocentric Ego4d domain. Below, we show some generated video clips with 32 frames to show the video prediction performance of our method.
            <div class="content">
                  <div class="columns is-centered">
            <video id="dollyzoom" width="1000" loop autoplay muted playsinline>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/ego.mp4" type="video/mp4">
                </video>
                  </div>
                  </div>
         
        </div>
          </div>
          
            <div class="content align-text">
                  
                    After the pre-training, VPDD exhibits the capability to generate future video clips across diverse tasks while maintaining dynamic consistency.
                    <strong>Each synthesized video clip consists of 4 frames, and serves as a strong guidance to enable efficient policy learning for downstream tasks.</strong>
                    <p style="color: red;display: inline;">
                      To demonstrate the dynamic consistency and accurate prediction of future behavior in the generated video clips by VPDD \(p_{\theta_1}\), we conduct multiple inferences and concatenate the resulting video clips to create an 8-second video. Below, we provide some examples observed in MetaWorld:
                      <video width="1000" loop autoplay muted>
                        <source src="./static/videos/MetaWorld.mp4" type="video/mp4">
                    </video>
                    We additionally fine-tune our pre-trained \(p_{\theta_1}\) on real robot videos with various embodiments to demonstrate the efficacy of our method in real environments. Subsequently, we concatenate the predicted video clips to formulate a 16-second coherent video. Below, we provide examples observed in the real environment:
                    <video width="1000" loop autoplay muted>
                      <source src="./static/videos/real.mp4" type="video/mp4">
                  </video>
                    </p>
            </div>
            <hr>
            <div class="content">

            

            Below, we illustrate some video clips on Meta-World. 


                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_meta_1.mp4" type="video/mp4">
                </video>
                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_meta_2.mp4" type="video/mp4">
                </video>
                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_meta_3.mp4" type="video/mp4">
                </video>

            </div>
            <hr>
            <div class="content align-text">
                  <p>
                   VPDD also demonstrates the ability to generate multi-view videos while ensuring view correspondence. Below, we illustrate some video clips on RLBench.
                  </p>
            </div>
            <div class="content">


                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_rlbench_1.mp4" type="video/mp4">
                </video>
                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_rlbench_2.mp4" type="video/mp4">
                </video>
                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_rlbench_3.mp4" type="video/mp4">
                </video>
                <video width="1000" loop autoplay muted>
                    <source src="https://github.com/Video-Diff/Video-Diff.github.io/raw/main/static/videos/_rlbench_4.mp4" type="video/mp4">
                </video>

            </div>
        </div>
      </section>

      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@article{2024video-diff,
         author    = {Anonymous Authors},
         title     = {Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training},
         year      = {2024},
}</code></pre>
        </div>
      </section>

      <footer class="footer">
        <div class="container">
          <!-- <div class="content has-text-centered">
            <a class="icon-link"
              href="./static/videos/nerfies_paper.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
          </div> -->
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                  <p>
                      This website is adapted from<a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>, licensed
                      under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License.</a>
                    </p>
              </div>
            </div>
          </div>
        </div>
      </footer>
  </div>

</body>
</html>
